# W&B Sweep Configuration for Hyperbolic CV Hyperparameter Search
# Usage: wandb sweep sweep_config.yaml

program: code/classification/train.py
method: bayes  # Options: grid, random, bayes
metric:
  name: val/loss
  goal: minimize

# Early termination to save compute
# early_terminate:
#   type: hyperband
#   min_iter: 15        # Don't stop runs before epoch 15
#   eta: 2              # Fraction of runs to keep at each elimination
#   s: 3                # Number of rungs in the bracket

parameters:
  # Fixed parameters (don't search these)
  dataset:
    value: CIFAR-100

  output_dir:
    value: null

  encoder_manifold:
    value: lorentz

  decoder_manifold:
    value: lorentz

  num_layers:
    value: 18

  embedding_dim:
    value: 512

  lr_scheduler:
    value: cosine

  val_fraction:
    value: 0.2  # Use proper train/val split

  # Seed for reproducibility
  seed:
    value: 1

  # Reduced training for hyperparameter search
  num_epochs:
    value: 75  # Instead of 200

  batch_size:
    value: 128

  batch_size_test:
    value: 128

  learn_k:
    value: False
  
  train_subset_fraction:
    value: 0.3
  
  clip_features:
    value: 1.0

  warmup_epochs:
    value: 10

  encoder_k:
    value: 1.0

  decoder_k:
    value: 1.0

  # Hyperparameters to search

  optimizer:
    values: [RiemannianAdam, RiemannianSGD]
  lr:
    distribution: log_uniform_values
    min: 3e-3
    max: 1.5e-1

  # Weight decay - log scale search
  weight_decay:
    distribution: log_uniform_values
    min: 5e-5
    max: 1e-3

  init_method:
    values: [old, eye1, eye05]