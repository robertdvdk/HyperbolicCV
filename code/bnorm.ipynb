{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e5f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.lorentz.layers import LorentzBatchNorm1d, LorentzBatchNorm2d\n",
    "from lib.lorentz.manifold import CustomLorentz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27399384",
   "metadata": {},
   "outputs": [],
   "source": [
    "man = CustomLorentz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2660da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Diagnostic script to visualize Lorentz BatchNorm train/eval discrepancy.\n",
    "\n",
    "Run this in your environment where lib.lorentz is available.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lib.lorentz.layers import LorentzBatchNorm1d, LorentzBatchNorm2d\n",
    "from lib.lorentz.manifold import CustomLorentz\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def generate_lorentz_points(manifold, batch_size, channels, height=None, width=None, spread=0.5):\n",
    "    \"\"\"Generate random points on the hyperboloid.\"\"\"\n",
    "    if height is not None and width is not None:\n",
    "        # For 2D: [B, C, H, W] where C includes time dimension\n",
    "        shape = (batch_size, channels, height, width)\n",
    "    else:\n",
    "        # For 1D: [B, C]\n",
    "        shape = (batch_size, channels)\n",
    "    \n",
    "    # Generate in tangent space at origin, then project\n",
    "    space_components = torch.randn(*shape) * spread\n",
    "    # Time component to satisfy hyperboloid constraint: t^2 - ||x||^2 = 1/k\n",
    "    # For k=1: t = sqrt(1 + ||x||^2)\n",
    "    if len(shape) == 4:\n",
    "        space_norm_sq = (space_components[:, 1:, :, :] ** 2).sum(dim=1, keepdim=True)\n",
    "    else:\n",
    "        space_norm_sq = (space_components[:, 1:] ** 2).sum(dim=1, keepdim=True)\n",
    "    \n",
    "    time_component = torch.sqrt(1.0 + space_norm_sq)\n",
    "    \n",
    "    if len(shape) == 4:\n",
    "        space_components[:, 0:1, :, :] = time_component\n",
    "    else:\n",
    "        space_components[:, 0:1] = time_component\n",
    "    \n",
    "    return space_components\n",
    "\n",
    "\n",
    "def shift_points(manifold, points, shift_amount, dim=1):\n",
    "    \"\"\"Shift points in a given spatial direction.\"\"\"\n",
    "    shifted = points.clone()\n",
    "    # Add to spatial component\n",
    "    if len(points.shape) == 4:\n",
    "        shifted[:, dim, :, :] += shift_amount\n",
    "        # Recompute time to stay on manifold\n",
    "        space_norm_sq = (shifted[:, 1:, :, :] ** 2).sum(dim=1, keepdim=True)\n",
    "        shifted[:, 0:1, :, :] = torch.sqrt(1.0 + space_norm_sq)\n",
    "    else:\n",
    "        shifted[:, dim] += shift_amount\n",
    "        space_norm_sq = (shifted[:, 1:] ** 2).sum(dim=1, keepdim=True)\n",
    "        shifted[:, 0:1] = torch.sqrt(1.0 + space_norm_sq)\n",
    "    return shifted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff4d9c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST: LorentzBatchNorm1d Train vs Eval Discrepancy\n",
      "======================================================================\n",
      "\n",
      "--- Training Phase (5 batches with drift) ---\n",
      "torch.Size([32, 16])\n",
      "Batch 0: input_space_norm=1.0538, output_space_norm=1.1903\n",
      "torch.Size([32, 16])\n",
      "Batch 1: input_space_norm=1.1937, output_space_norm=1.1892\n",
      "torch.Size([32, 16])\n",
      "Batch 2: input_space_norm=1.2777, output_space_norm=1.1854\n",
      "torch.Size([32, 16])\n",
      "Batch 3: input_space_norm=1.4315, output_space_norm=1.1880\n",
      "torch.Size([32, 16])\n",
      "Batch 4: input_space_norm=1.6288, output_space_norm=1.1858\n",
      "\n",
      "--- Eval Phase (same distribution as last training batch) ---\n",
      "\n",
      "Same input, different modes:\n",
      "  Input space norm:        1.6486\n",
      "  Output (train mode):     1.1864\n",
      "  Output (eval mode):      1.4896\n",
      "\n",
      "  Point-wise difference (L2): mean=0.9616, max=1.1411\n",
      "\n",
      "--- Manifold Constraint Check ---\n",
      "  Input manifold constraint (should be ~1): mean=1.0000, std=0.000000\n",
      "  Output (train) manifold constraint (should be ~1): mean=1.0000, std=0.000000\n",
      "  Output (eval) manifold constraint (should be ~1): mean=1.0000, std=0.000000\n"
     ]
    }
   ],
   "source": [
    "def test_train_eval_discrepancy_1d():\n",
    "    \"\"\"Test 1D BatchNorm discrepancy.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TEST: LorentzBatchNorm1d Train vs Eval Discrepancy\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    manifold = CustomLorentz()\n",
    "    channels = 16  # includes time dimension\n",
    "    bn = LorentzBatchNorm1d(manifold, channels)  # -1 because channels includes time\n",
    "    \n",
    "    # Simulate training with drifting data\n",
    "    print(\"\\n--- Training Phase (5 batches with drift) ---\")\n",
    "    bn.train()\n",
    "    \n",
    "    for i in range(5):\n",
    "        # Generate batch, shift more each iteration\n",
    "        batch = generate_lorentz_points(manifold, 32, channels, spread=0.3)\n",
    "        batch = shift_points(manifold, batch, shift_amount=0.3 * i, dim=1)\n",
    "        print(batch.shape)\n",
    "        \n",
    "        output = bn(batch, momentum=0.1)\n",
    "        \n",
    "        # Compute output statistics\n",
    "        space_norms = torch.norm(output[:, 1:], dim=1)\n",
    "        print(f\"Batch {i}: input_space_norm={torch.norm(batch[:, 1:], dim=1).mean():.4f}, \"\n",
    "              f\"output_space_norm={space_norms.mean():.4f}\")\n",
    "    \n",
    "    # Now test in eval mode on similar data\n",
    "    print(\"\\n--- Eval Phase (same distribution as last training batch) ---\")\n",
    "    bn.eval()\n",
    "    \n",
    "    # Generate test data similar to last training batch\n",
    "    test_batch = generate_lorentz_points(manifold, 32, channels, spread=0.3)\n",
    "    test_batch = shift_points(manifold, test_batch, shift_amount=0.3 * 4, dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get output in eval mode\n",
    "        output_eval = bn(test_batch, momentum=0.1)\n",
    "        \n",
    "        # For comparison, temporarily switch to train mode\n",
    "        bn.train()\n",
    "        output_train = bn(test_batch, momentum=0.1)\n",
    "        bn.eval()\n",
    "    \n",
    "    print(f\"\\nSame input, different modes:\")\n",
    "    print(f\"  Input space norm:        {torch.norm(test_batch[:, 1:], dim=1).mean():.4f}\")\n",
    "    print(f\"  Output (train mode):     {torch.norm(output_train[:, 1:], dim=1).mean():.4f}\")\n",
    "    print(f\"  Output (eval mode):      {torch.norm(output_eval[:, 1:], dim=1).mean():.4f}\")\n",
    "    \n",
    "    # Check point-wise difference\n",
    "    diff = torch.norm(output_train - output_eval, dim=1)\n",
    "    print(f\"\\n  Point-wise difference (L2): mean={diff.mean():.4f}, max={diff.max():.4f}\")\n",
    "    \n",
    "    # Check if outputs are on manifold\n",
    "    def check_manifold(x, name):\n",
    "        # Should satisfy t^2 - ||s||^2 = 1\n",
    "        constraint = x[:, 0]**2 - (x[:, 1:]**2).sum(dim=1)\n",
    "        print(f\"  {name} manifold constraint (should be ~1): mean={constraint.mean():.4f}, std={constraint.std():.6f}\")\n",
    "    \n",
    "    print(\"\\n--- Manifold Constraint Check ---\")\n",
    "    check_manifold(test_batch, \"Input\")\n",
    "    check_manifold(output_train, \"Output (train)\")\n",
    "    check_manifold(output_eval, \"Output (eval)\")\n",
    "test_train_eval_discrepancy_1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a73ec53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: LorentzBatchNorm2d Train vs Eval Discrepancy\n",
      "======================================================================\n",
      "\n",
      "--- Training Phase (5 batches with drift) ---\n",
      "Batch 0: output_space_norm=0.9965\n",
      "Batch 1: output_space_norm=0.9958\n",
      "Batch 2: output_space_norm=0.9939\n",
      "Batch 3: output_space_norm=0.9976\n",
      "Batch 4: output_space_norm=1.0243\n",
      "\n",
      "--- Eval Phase ---\n",
      "\n",
      "Same input, different modes:\n",
      "  Output (train mode): space_norm=1.0023\n",
      "  Output (eval mode):  space_norm=1.2545\n",
      "  Ratio (eval/train): 1.2517\n"
     ]
    }
   ],
   "source": [
    "def test_train_eval_discrepancy_2d():\n",
    "    \"\"\"Test 2D BatchNorm discrepancy.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST: LorentzBatchNorm2d Train vs Eval Discrepancy\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    manifold = CustomLorentz()\n",
    "    channels = 8  # includes time dimension\n",
    "    height, width = 4, 4\n",
    "    bn = LorentzBatchNorm2d(manifold, channels)\n",
    "    \n",
    "    # Simulate training\n",
    "    print(\"\\n--- Training Phase (5 batches with drift) ---\")\n",
    "    bn.train()\n",
    "    \n",
    "    for i in range(5):\n",
    "        batch = generate_lorentz_points(manifold, 16, channels, height, width, spread=0.3)\n",
    "        batch = shift_points(manifold, batch, shift_amount=0.5 * i, dim=1)\n",
    "\n",
    "        batch = batch.permute(0, 2, 3, 1)\n",
    "        \n",
    "        output = bn(batch, momentum=0.1)\n",
    "        \n",
    "        space_norms = torch.norm(output[:, 1:], dim=1).mean()\n",
    "        print(f\"Batch {i}: output_space_norm={space_norms:.4f}\")\n",
    "    \n",
    "    # Eval mode\n",
    "    print(\"\\n--- Eval Phase ---\")\n",
    "    bn.eval()\n",
    "    \n",
    "    test_batch = generate_lorentz_points(manifold, 16, channels, height, width, spread=0.3)\n",
    "    test_batch = shift_points(manifold, test_batch, shift_amount=0.5 * 4, dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_batch = test_batch.permute(0, 2, 3, 1)\n",
    "        output_eval = bn(test_batch, momentum=0.1)\n",
    "        \n",
    "        bn.train()\n",
    "        output_train = bn(test_batch, momentum=0.1)\n",
    "        bn.eval()\n",
    "    \n",
    "    print(f\"\\nSame input, different modes:\")\n",
    "    print(f\"  Output (train mode): space_norm={torch.norm(output_train[:, 1:], dim=1).mean():.4f}\")\n",
    "    print(f\"  Output (eval mode):  space_norm={torch.norm(output_eval[:, 1:], dim=1).mean():.4f}\")\n",
    "    \n",
    "    ratio = torch.norm(output_eval[:, 1:], dim=1).mean() / torch.norm(output_train[:, 1:], dim=1).mean()\n",
    "    print(f\"  Ratio (eval/train): {ratio:.4f}\")\n",
    "test_train_eval_discrepancy_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63f20765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Running Statistics Analysis\n",
      "======================================================================\n",
      "\n",
      "--- Tracking running statistics during training ---\n",
      "Batch 0: running_var=0.9973, running_mean_norm=0.0166\n",
      "Batch 1: running_var=0.9920, running_mean_norm=0.0209\n",
      "Batch 2: running_var=0.9912, running_mean_norm=0.0399\n",
      "Batch 3: running_var=0.9900, running_mean_norm=0.0747\n",
      "Batch 4: running_var=0.9825, running_mean_norm=0.1196\n",
      "Batch 5: running_var=0.9810, running_mean_norm=0.1699\n",
      "Batch 6: running_var=0.9779, running_mean_norm=0.2260\n",
      "Batch 7: running_var=0.9703, running_mean_norm=0.2896\n",
      "Batch 8: running_var=0.9672, running_mean_norm=0.3587\n",
      "Batch 9: running_var=0.9632, running_mean_norm=0.4264\n",
      "\n",
      "--- Final running statistics ---\n",
      "running_mean: tensor([ 0.0000,  0.4256,  0.0081, -0.0056,  0.0031])...\n",
      "running_var: tensor([0.9632])\n"
     ]
    }
   ],
   "source": [
    "def test_running_stats_analysis():\n",
    "    \"\"\"Analyze what's stored in running statistics.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST: Running Statistics Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    manifold = CustomLorentz()\n",
    "    channels = 16\n",
    "    bn = LorentzBatchNorm1d(manifold, channels)\n",
    "    \n",
    "    bn.train()\n",
    "    \n",
    "    print(\"\\n--- Tracking running statistics during training ---\")\n",
    "    \n",
    "    for i in range(10):\n",
    "        batch = generate_lorentz_points(manifold, 32, channels, spread=0.3)\n",
    "        batch = shift_points(manifold, batch, shift_amount=0.2 * i, dim=1)\n",
    "        \n",
    "        # Before forward pass\n",
    "        running_mean_before = bn.running_mean.clone() if hasattr(bn, 'running_mean') else None\n",
    "        running_var_before = bn.running_var.clone() if hasattr(bn, 'running_var') else None\n",
    "        \n",
    "        output = bn(batch, momentum=0.1)\n",
    "        \n",
    "        # After forward pass\n",
    "        if hasattr(bn, 'running_mean') and hasattr(bn, 'running_var'):\n",
    "            print(f\"Batch {i}: running_var={bn.running_var.item():.4f}, \"\n",
    "                  f\"running_mean_norm={torch.norm(bn.running_mean):.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Final running statistics ---\")\n",
    "    if hasattr(bn, 'running_mean'):\n",
    "        print(f\"running_mean: {bn.running_mean[:5]}...\")  # First 5 elements\n",
    "    if hasattr(bn, 'running_var'):\n",
    "        print(f\"running_var: {bn.running_var}\")\n",
    "test_running_stats_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d8a0925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Extreme Distribution Shift\n",
      "======================================================================\n",
      "\n",
      "--- Training on data near origin ---\n",
      "After training: running_var=0.9734\n",
      "\n",
      "--- Evaluating on data far from origin ---\n",
      "Shift=0: output_space_norm=1.1924\n",
      "Shift=1: output_space_norm=1.5842\n",
      "Shift=2: output_space_norm=2.4275\n",
      "Shift=5: output_space_norm=5.4845\n",
      "Shift=10: output_space_norm=10.9300\n"
     ]
    }
   ],
   "source": [
    "def test_extreme_drift():\n",
    "    \"\"\"Test with extreme distribution shift between train and eval.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST: Extreme Distribution Shift\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    manifold = CustomLorentz()\n",
    "    channels = 16\n",
    "    bn = LorentzBatchNorm1d(manifold, channels)\n",
    "    \n",
    "    # Train on data near origin\n",
    "    print(\"\\n--- Training on data near origin ---\")\n",
    "    bn.train()\n",
    "    for i in range(10):\n",
    "        batch = generate_lorentz_points(manifold, 32, channels, spread=0.3)\n",
    "        output = bn(batch, momentum=0.1)\n",
    "    \n",
    "    print(f\"After training: running_var={bn.running_var.item():.4f}\")\n",
    "    \n",
    "    # Eval on data FAR from origin\n",
    "    print(\"\\n--- Evaluating on data far from origin ---\")\n",
    "    bn.eval()\n",
    "    \n",
    "    for shift in [0, 1, 2, 5, 10]:\n",
    "        test_batch = generate_lorentz_points(manifold, 32, channels, spread=0.3)\n",
    "        test_batch = shift_points(manifold, test_batch, shift_amount=shift, dim=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = bn(test_batch, momentum=0.1)\n",
    "        \n",
    "        output_norm = torch.norm(output[:, 1:], dim=1).mean()\n",
    "        print(f\"Shift={shift}: output_space_norm={output_norm:.4f}\")\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        if torch.isnan(output).any() or torch.isinf(output).any():\n",
    "            print(f\"  WARNING: NaN or Inf detected!\")\n",
    "test_extreme_drift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72aab69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Validation Noise Simulation (Cosine Annealing Scenario)\n",
      "======================================================================\n",
      "\n",
      "--- Simulating training with evolving features ---\n",
      "\n",
      "Epoch | Train Output Norm | Eval Output Norm | Ratio\n",
      "------------------------------------------------------------\n",
      "   0  |      1.1883       |      1.1654      | 0.9807\n",
      "   1  |      1.1844       |      1.2229      | 1.0326\n",
      "   2  |      1.1851       |      1.3712      | 1.1571\n",
      "   3  |      1.1827       |      1.4078      | 1.1904\n",
      "   4  |      1.1872       |      1.3955      | 1.1755\n",
      "   5  |      1.1850       |      1.5399      | 1.2995\n",
      "   6  |      1.1889       |      1.5695      | 1.3201\n",
      "   7  |      1.1848       |      1.5182      | 1.2814\n",
      "   8  |      1.1874       |      1.4906      | 1.2553\n",
      "   9  |      1.1818       |      1.5970      | 1.3513\n",
      "  10  |      1.1868       |      1.6039      | 1.3514\n",
      "  11  |      1.1827       |      1.6135      | 1.3642\n",
      "  12  |      1.1852       |      1.5791      | 1.3323\n",
      "  13  |      1.1819       |      1.6097      | 1.3619\n",
      "  14  |      1.1817       |      1.5542      | 1.3152\n",
      "  15  |      1.1805       |      1.6613      | 1.4073\n",
      "  16  |      1.1802       |      1.5858      | 1.3437\n",
      "  17  |      1.1801       |      1.5339      | 1.2999\n",
      "  18  |      1.1831       |      1.5370      | 1.2991\n",
      "  19  |      1.1836       |      1.5316      | 1.2941\n",
      "\n",
      ">>> Notice how the ratio diverges as training progresses!\n",
      ">>> This is because running stats become increasingly stale.\n"
     ]
    }
   ],
   "source": [
    "def test_validation_noise_simulation():\n",
    "    \"\"\"\n",
    "    Simulate the validation noise you're seeing during cosine annealing.\n",
    "    \n",
    "    The hypothesis: validation uses eval mode (running stats) while\n",
    "    training uses train mode (batch stats). During cosine annealing,\n",
    "    the model changes but running stats become stale.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST: Validation Noise Simulation (Cosine Annealing Scenario)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    manifold = CustomLorentz()\n",
    "    channels = 16\n",
    "    bn = LorentzBatchNorm1d(manifold, channels)\n",
    "    \n",
    "    # Simulate: features evolve during training (as weights change)\n",
    "    # but running stats lag behind\n",
    "    \n",
    "    print(\"\\n--- Simulating training with evolving features ---\")\n",
    "    \n",
    "    train_outputs = []\n",
    "    eval_outputs = []\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        # Feature distribution evolves (simulating effect of changing weights)\n",
    "        feature_scale = 0.3 + 0.02 * epoch  # gradually increasing\n",
    "        feature_shift = 0.1 * epoch  # gradually drifting\n",
    "        \n",
    "        # Training step\n",
    "        bn.train()\n",
    "        batch = generate_lorentz_points(manifold, 32, channels, spread=feature_scale)\n",
    "        batch = shift_points(manifold, batch, shift_amount=feature_shift, dim=1)\n",
    "        train_out = bn(batch, momentum=0.1)\n",
    "        train_outputs.append(torch.norm(train_out[:, 1:], dim=1).mean().item())\n",
    "        \n",
    "        # Validation step (same data distribution, but eval mode)\n",
    "        bn.eval()\n",
    "        val_batch = generate_lorentz_points(manifold, 32, channels, spread=feature_scale)\n",
    "        val_batch = shift_points(manifold, val_batch, shift_amount=feature_shift, dim=1)\n",
    "        with torch.no_grad():\n",
    "            eval_out = bn(val_batch, momentum=0.1)\n",
    "        eval_outputs.append(torch.norm(eval_out[:, 1:], dim=1).mean().item())\n",
    "    \n",
    "    print(\"\\nEpoch | Train Output Norm | Eval Output Norm | Ratio\")\n",
    "    print(\"-\" * 60)\n",
    "    for i in range(20):\n",
    "        ratio = eval_outputs[i] / train_outputs[i] if train_outputs[i] > 0 else float('inf')\n",
    "        print(f\"  {i:2d}  |      {train_outputs[i]:.4f}       |      {eval_outputs[i]:.4f}      | {ratio:.4f}\")\n",
    "    \n",
    "    print(\"\\n>>> Notice how the ratio diverges as training progresses!\")\n",
    "    print(\">>> This is because running stats become increasingly stale.\")\n",
    "test_validation_noise_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HCNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
